results = [] # Initialising various lists to store performance metrics
performance = []
method = []
times = []

# function for fitting a model
def fitmodel(X, pheno, estimator, parameters, modelname, method, performance, times) :
    kfold = KFold(n_splits=5) # Performing Cross validaion using Kfold cross vallidation technique

    for train_index, test_index in kfold.split(X, pheno):# Returns indices of the split training ad test data
        
        start = time.process_time() # time how long it takes to train each model type
        
        # split data into train/test sets
        X_train = X.iloc[train_index]
        y_train = pheno[train_index]
        X_test = X.iloc[test_index]
        y_test = pheno[test_index]
        
        # GridSearch Cross Validation builds multiple models using different combinations of hyperparameters and sees which combination performs the best.
        gs_clf = GridSearchCV(estimator=estimator, param_grid=parameters, cv=3, n_jobs=-1, scoring='balanced_accuracy')
        # cv mentions number of cross validations the model will perform with each set of hyperparameters, n_jobs is assigned to use all available processors
        
        gs_clf.fit(X_train, y_train) #fit each training and test set
        y_pred = gs_clf.predict(X_test)# predict resistance in test set
        
        y_pred[y_pred<0.5] = 0 #assign binary values to non binary predictions
        y_pred[y_pred>0.5] = 1
        results= np.append (y_pred, y_test) #store results for comparison

        score = balanced_accuracy_score(y_test, y_pred) # Measure balanced accuracy scores
        performance = np.append(performance, score)
        method = np.append(method, modelname)# Store the performance, method and times
        times = np.append(times, (time.process_time() - start))

        print("Best hyperparameters for this fold")
        print(gs_clf.best_params_)
    return gs_clf, method, performance, times, results
